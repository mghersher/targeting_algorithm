from geopy import distance as geo_dist

from math import isnan

from mapsplotlib import mapsplot as mplt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import matplotlib.patches as mpatches
import sklearn.preprocessing
from math import isnan

from ipywidgets.embed import embed_minimal_html
import gmaps
import gmaps.geojson_geometries
import json

import random

def compile_original_results(original_df):
    '''
    Concatenates clustering results of each block level Kmeans/IP run into one dataframe with
    continuous cluster IDs; Also concatenates the 3 summary stats dataframes generated by
    the Kmeans/IP code and the cluster centers
    
    Parameters:
    original_df: Original raw village dataframe (to extract the block names of the relevant 
    3 districts and load each file from the Kmeans/IP run)
    '''
    #All districts EG wants to expand into
    districts_select = ["KHANDWA", "KHARGONE", "SHIVPURI"]
    #Min number of OOSC required in IP for small clusters for each district
    village_min_vals = [175, 230, 175]

    blocks_dict = {}
    stats_dict = {}
    dists_dict = {}
    OOSC_dict = {}
    centers_dict = {}
    cluster_count = 0
    #Generate list of all unique block names in district
    for district_name, village_min in zip(districts_select, village_min_vals):
        district = original_df[original_df.District == district_name]
        blocks = district.Block.unique()
        
        blocks_already = ["KHALWA", "BHAGBANPURA", "NARVAR"]
        blocks_new = [block for block in blocks if block not in blocks_already]

        for block_name in blocks_new:
            #----------------Load cluster results--------------#
            file = "Excel_results/Kmeans_IP/{}_{}_distweight0.4_distcap20_villcap8_villagemin4_small_cluster_OOSC_min{}.xlsx".format(district_name, block_name, village_min)
            block_df = pd.read_excel(file, sheet_name = "Cluster_results")
            num_clusters = len(block_df.cluster_id.unique()) - 1

            #Shift all nonzero cluster_id's up by the cluster count up to this point to prevent cluster_id collissions when plotting
            block_df.loc[block_df.cluster_id != 0, "cluster_id"] += cluster_count

            #Save block dataframe with shifted cluster_ids in dict for merging later
            blocks_dict[block_name] = block_df

            #-------------Load cluster stats-------------#
            stats_df = pd.read_excel(file, sheet_name = "Cluster_stats")
            
            #Shift all nonzero cluster_id's up by the cluster count up to this point to prevent cluster_id collissions when plotting
            stats_df.loc[stats_df.clust_num != 0, "clust_num"] += cluster_count
            stats_dict[block_name] = stats_df

            #Generate column names for other cluster stats sheets
            column_names = ["Cluster_{}".format(i) for i in range(num_clusters + 1)]

            dists_df = pd.read_excel(file, sheet_name = "Cluster_dists", names = column_names, skiprows = [1])
            column_shifted_names = ["Cluster_0"] + ["Cluster_{}".format(shifted_cluster) for shifted_cluster in range(cluster_count + 1, num_clusters + cluster_count + 1)]
            dists_df.columns = column_shifted_names
            dists_dict[block_name] = dists_df

            OOSC_df = pd.read_excel(file, sheet_name = "OOSC_numbers", names = column_names, skiprows = [1])
            OOSC_df.columns = column_shifted_names
            OOSC_dict[block_name] = OOSC_df
            
            centers_df = pd.read_excel(file, sheet_name = "Cluster_centers")
            centers_df.reset_index(level=0, inplace=True)
            centers_df.columns = ["clust_num", "GPS_lat", "GPS_long"]
            centers_df.loc[centers_df.clust_num != 0, "clust_num"] += cluster_count
            centers_dict[block_name] = centers_df

            #Increase the cluster_count by the number of clusters in this block for the shifting of cluster IDs
            cluster_count += num_clusters

        cluster_results = pd.concat(blocks_dict.values(), ignore_index = True)

    #Use stats dictionaries to create merged stats pandas dataframes
    merged_stats = pd.concat(stats_dict.values(), ignore_index = True)
    cluster_stats = merged_stats[merged_stats.clust_num != 0]
    cluster_stats = cluster_stats.reset_index(drop = True)

    merged_dists = pd.concat(dists_dict.values(), axis=1)
    dist_stats = merged_dists.drop(["Cluster_0"], axis=1)
    dist_stats = dist_stats.dropna(axis=0,how = "all")

    merged_OOSC = pd.concat(OOSC_dict.values(), axis=1)
    OOSC_stats = merged_OOSC.drop(["Cluster_0"], axis=1)
    OOSC_stats = OOSC_stats.dropna(axis=0,how = "all")

    merged_centers = pd.concat(centers_dict.values(), ignore_index = True)
    cluster_centers = merged_centers[merged_centers.clust_num != 0]
    cluster_centers = cluster_centers.reset_index(drop = True)


    return cluster_results, cluster_stats, dist_stats, OOSC_stats, cluster_centers

def EG_subset(original_df):
    '''
    Returns a dataframe which is the subset of villages EG is considering expanding into
    Output necessary for the analyze_output() function

    Parameters:
    original_df: Original raw village dataframe
    '''
    districts_select = ["KHANDWA", "KHARGONE", "SHIVPURI"]
    blocks_already = ["KHALWA", "BHAGBANPURA", "NARVAR"]
    EG_districts = original_df.loc[(original_df["District"].isin(districts_select)) & (~original_df["Block"].isin(blocks_already))]

    return EG_districts

def analyze_output(EG_districts_, cluster_stats_, dist_stats_):
    '''
    Displays important diagnostic summary statistics to the console

    Parameters:
    EG_districts_: dataframe of the subset of villages EG is interested in (EG_subset() function output)
    clusters_stats_, dist_stats: cluster summary and distance statistics dataframes generated by Kmeans/IP algorithm final_stats() 
    function or recompute_final_stats() function
    '''
    #Number of OOSC in district
    total_OOSC_across_states = np.sum(EG_districts_["OOSCpredicted"].values)
    print("Total number of OOSC across viable villages in 3 EG states:", total_OOSC_across_states)

    print("The current clustering solution achieves the following results:")
    #Number of OOSC reached
    total_OOSC = np.sum(cluster_stats_.Total_OOSC)
    print("Number of OOSC reached:", total_OOSC)

    #Percentage of possible OOSC reached
    percent_OOSC_reached = total_OOSC/total_OOSC_across_states
    print("Percent of possible OOSC reached:", percent_OOSC_reached)

    #Number of villages clustered
    num_villages = np.sum(cluster_stats_.Num_villages)
    print("Total number of villages clustered:", num_villages)

    #Average number of OOSC in clusters
    avg_OOSC = np.mean(cluster_stats_.Total_OOSC)
    print("Average number of OOSC in clusters:", avg_OOSC)

    var_OOSC = np.var(cluster_stats_.Total_OOSC)
    print("Variance in number of OOSC in clusters:", var_OOSC)

    max_OOSC = np.max(cluster_stats_.Total_OOSC)
    print("Maximum number of OOSC in a cluster:", max_OOSC)

    min_OOSC = np.min(cluster_stats_.Total_OOSC)
    print("Minimum number of OOSC in a cluster:", min_OOSC)

    #Average number of OOSC in clusters
    avg_dist = np.mean(cluster_stats_.mean_dist)
    print("Average village-centroid distance in clusters:", avg_dist)

    max_dist = np.nanmax(dist_stats_.values)
    print("Maximum village-centroid distance in a cluster:", max_dist)

    return

def stats_figures(dist_stats_, cluster_stats_, postprocessing_stage):
    '''
    Generates histograms of total OOSC children at the cluster level, village-centroid distances,
    and cluster sizes

    Parameters:
    EG_districts_: dataframe of the subset of villages EG is interested in (EG_subset() function output)
    clusters_stats_, dist_stats: cluster summary and distance statistics dataframes generated by Kmeans/IP algorithm final_stats() 
    function or recompute_final_stats() function
    postprocessing_stage: step of postprocessing to generate figure file names
    '''

    plt.close('all')
    fig, (ax1,ax2,ax3) = plt.subplots(3, 1, figsize=(10,10))

    # Plot each graph, and manually set the y tick values
    ax1.hist(cluster_stats_.Total_OOSC.values, bins = 50)
    ax1.set_title("Distribution of OOSC across clusters")
    ax1.set_xlabel('Number of OOSC in cluster')

    dists = [elem for sublist in dist_stats_.values for elem in sublist if not isnan(elem)]
    ax2.hist(dists, bins = 50)
    ax2.set_title("Distribution of village-centroid distances")
    ax2.set_xlabel('Village-centroid distance')


    ax3.hist(cluster_stats_.Num_villages, bins = 40)
    ax3.set_title("Distribution of cluster size")
    ax3.set_xlabel('Number of villages in cluster')

    plt.subplots_adjust(hspace = 1)
    plt.savefig("stats_figures/{}_hists.png".format(postprocessing_stage))
    
    return

def distance_calc(datapoint, center):
    '''Calculates distance between two points for use in below functions

    Parameters:
    datapoint: a lat/long tuple corresponding to a village
    center: a lat/long tuple corresponding to a cluster center
    '''
    return geo_dist.vincenty(datapoint, center).km

def nearest_centroid(village_coords, centers_df_):
    '''Assigns villages in village_coords to closest centroid
    Returns list of the cluster assignments in a list the length of village_coords
    '''
    #Drop cluster ID for 0th 'fake' cluster
    centers_df_real = centers_df_[centers_df_.cluster_id != 0]
    #Calculate the new closest centroid for these villages which are far away from their current cluster centers
    new_clusters = []
    for village in village_coords:
        dist_dict = {}
        for index, row in centers_df_.iterrows():
            dist_dict[row.cluster_id] = distance_calc(village, row.centroid)
        new_clusters.append(min(dist_dict, key=dist_dict.get))

    return new_clusters

def postprocessing_small(cluster_results_, cluster_stats_, cluster_centers_):
    '''
    Dissolves clusters with 4 or less villages and assign these villages to closest cluster

    Parameters:
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    clusters_stats_, dist_stats: cluster summary and distance statistics dataframes generated by Kmeans/IP algorithm final_stats() 
    function or recompute_final_stats() function
    '''
    #Extract cluster IDs for small clusters
    small_clusters = cluster_stats_[cluster_stats_.Num_villages <= 4]
    small_clust_ids = list(small_clusters.clust_num.values)

    #Select the clustering results for the villages in clusters with less than 4 villages
    small_clusters = cluster_results_.loc[cluster_results_.cluster_id.isin(small_clust_ids)]
    small_cluster_coords = list(zip(small_clusters.GPS_lat, small_clusters.GPS_long))
    print("Number of villages being reassigned:", len(small_cluster_coords))

    #Dissolve small clusters and reassign these villages to remaining nearest centroid
    #Centroids remaining once you dissolve small clusters
    centroids = cluster_centers_[~cluster_centers_.clust_num.isin(small_clust_ids)]
    centroid_coords = list(zip(centroids.GPS_lat, centroids.GPS_long))
    centroid_df = pd.DataFrame({"cluster_id":list(centroids.clust_num), "centroid":centroid_coords})

    #Reassign villages in small clusters to closest real centroid
    new_clusters = nearest_centroid(village_coords = small_cluster_coords, centers_df_ = centroid_df)

    #Merge these new cluster assignments back with original cluster assignments
    large_clusters = cluster_results_.loc[~cluster_results_.cluster_id.isin(small_clust_ids)]
    small_clusters_new = pd.DataFrame({"District": small_clusters.District, "Block":small_clusters.Block, "Village":small_clusters.Village, "VillageCode":small_clusters.VillageCode, "GPS_lat":small_clusters.GPS_lat, "GPS_long":small_clusters.GPS_long, "OOSCpredicted":small_clusters.OOSCpredicted, "cluster_id":new_clusters})

    merged_clusters_new = pd.concat([large_clusters, small_clusters_new], ignore_index = True)

    #Track which clusters are being dropped
    clusters_dropped = pd.DataFrame({"Clusters_dropped":small_clust_ids})
    print("Clusters being dissolved:", small_clust_ids)

    #Write new results and clusters dropped to an excel file
    writer = pd.ExcelWriter("Excel_results/Postprocessed1/Postprocessed1_dropsmall.xlsx")
    merged_clusters_new.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    clusters_dropped.to_excel(excel_writer = writer, sheet_name = 'Clusters_dropped')
    writer.save()

    return

def compute_centers(cluster_results_):

    '''
    Generates a dataframe with columns: cluster_id and centroid (lat/long tuples of cluster centers)
    
    Parameters:
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    '''

    #Create dictionary with keys = cluster_ids and values = dataframe of lat/longs for items in that cluster
    cluster_ids = cluster_results_.cluster_id.unique()
    cluster_dict = {}
    for cluster_id in cluster_ids:
        cluster_df = cluster_results_[cluster_results_.cluster_id == cluster_id][["GPS_lat", "GPS_long"]]
        cluster_dict[cluster_id] = cluster_df

    '''Takes in dictionary with keys = cluster_ids and values = pandas array of two columns lat and long
    and outputs new cluster centers df
    '''
    centers = []
    for key,value in cluster_dict.items():
        centers.append((np.mean(value.GPS_lat), np.mean(value.GPS_long)))

    centers_df = pd.DataFrame({"cluster_id":list(cluster_dict.keys()), "centroid":centers})

    return centers_df

def recompute_final_stats(centers_, cluster_results_, clusters_dropped = []):
    '''
    Takes in new final clusters and centers and returns 3 pandas dataframes of summary statistics

    Dataframe 1 - Cluster level:

    clust_num: Cluster ID
    max_dist: Max village-centroid distance in cluster
    mean_dist: Average village-centroid distance in clusters
    var_dist: Variance of village-centroid distances in clusters
    Avg_OOSC: Avg OOSC in villages in clsuters 
    Total_OOSC: Total OOSC in cluster
    Num_villages: Number of villages in cluster

    Dataframe 2 - Cluster level: All village-centroid distances in cluster

    Dataframe 3 - Cluster level: Number of OOSC in each village in cluster

    Parameters:
    centers_: centers dataframe generated by compile_original_results() or compute_centers() functions
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    clusters_dropped: list of cluster_IDs that have been dropped making cluster IDs non-continuous
    '''
    #Number of clusters
    k = max(cluster_results_.cluster_id.unique()) + 1
    cluster_vals = list(range(k))
    cluster_vals_new = [val for val in cluster_vals if val not in clusters_dropped]

    dist_dict = {}
    OOSC_dict = {}
    max_dist_vals = np.zeros(len(cluster_vals_new))
    avg_dist_vals = np.zeros(len(cluster_vals_new))
    var_dist_vals = np.zeros(len(cluster_vals_new))
    OOSC_means = np.zeros(len(cluster_vals_new))
    OOSC_totals = np.zeros(len(cluster_vals_new))
    num_villages_cluster = np.zeros(len(cluster_vals_new))

    cluster_vals = list(range(k))
    cluster_vals_new = [val for val in cluster_vals if val not in clusters_dropped]

    for index, j in enumerate(cluster_vals_new):
        #Extract villages in cluster k
        villages_in_k = cluster_results_[cluster_results_.cluster_id == j]
        num_villages_in_k = villages_in_k.shape[0]
        num_villages_cluster[index] = num_villages_in_k

        #Calculate all distances between villages and cluster centroids
        villages_k_GPS = villages_in_k[["GPS_lat", "GPS_long"]]

        distances_villages = []
        for village in villages_k_GPS.values:
            dist_center = distance_calc(datapoint = village, center = centers_[centers_.cluster_id == j]["centroid"])
            distances_villages.append(dist_center)

        dist_dict["Cluster_{}".format(j)] = distances_villages

        #Calculate some stats about village-centroid distances within clusters
        distances_villages = np.asarray(distances_villages)
        max_dist_vals[index] = np.max(distances_villages)
        avg_dist_vals[index] = np.mean(distances_villages)
        var_dist_vals[index] = np.var(distances_villages)

        #Calculate OOSC stats within clusters
        villages_k_OOSC = villages_in_k[["OOSCpredicted"]]
        OOSC_means[index] = np.mean(villages_k_OOSC)
        OOSC_totals[index] = np.sum(villages_k_OOSC)
        OOSC_dict["Cluster_{}".format(j)] = [item for sublist in villages_k_OOSC.values for item in sublist] 
    
    #Combine all the stats into a pandas dataframe to return
    cluster_num = cluster_vals_new
    stats_df = pd.DataFrame({"clust_num":cluster_num,"max_dist": max_dist_vals, "mean_dist":avg_dist_vals, "var_dist":var_dist_vals, "Avg_OOSC": OOSC_means, "Total_OOSC": OOSC_totals, "Num_villages": num_villages_cluster})

    #Calculate total number of OOSC reached
    OOSC_overall_total = np.sum(stats_df["Total_OOSC"][1:])
    print("Number of OOSC children reached:", OOSC_overall_total)

    cluster_stats = stats_df[stats_df.clust_num != 0]

    values = list(dist_dict.values())
    keys = list(dist_dict.keys())
    dist_df= pd.DataFrame(data = values).T
    dist_df.columns = [keys]
    dist_df = dist_df.drop(["Cluster_0"], axis=1)
    dist_stats = dist_df.dropna(axis=0,how = "all")

    values = list(OOSC_dict.values())
    keys = list(OOSC_dict.keys())
    OOSC_df= pd.DataFrame(data = values).T
    OOSC_df.columns = [keys]
    OOSC_df = OOSC_df.drop(["Cluster_0"], axis=1)
    OOSC_stats = OOSC_df.dropna(axis=0, how = "all")

    return cluster_stats, dist_stats, OOSC_stats

def postprocessing_far(cluster_results_, centers_df):
    '''
    Uncluster villages who are 5 or more km from their cluster centers and reassign 
    them to closest cluster
    
    Parameters:
    centers_df: centers dataframe generated by compile_original_results() or compute_centers() functions
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    '''
    #Append cluster centers to cluster results dataframe
    centroids = []
    for index, row in cluster_results_.iterrows():
        id_val = row.cluster_id
        centroids.append(centers_df[centers_df.cluster_id == id_val].centroid.values[0])
    centroids = pd.DataFrame({"centroid": centroids})
    merged_df = pd.concat([cluster_results_, centroids], axis = 1)

    #Calculate village-centroid distance for each village and attach it to the village name
    dists = []
    for index, row in merged_df.iterrows():
        coord_tuple = (row.GPS_lat, row.GPS_long)
        dists.append(distance_calc(coord_tuple, row.centroid))
        
    dists = pd.DataFrame({"vill_centroid_dist": dists})
    full_df = pd.concat([merged_df, dists], axis = 1)

    '''Create a subset dataframe of the villages who are more than 5km away from their cluster centers and are not in fake cluster '''
    far_villages_df = full_df.loc[(full_df.vill_centroid_dist >= 5) & (full_df.cluster_id != 0)]
    print("Number of villages reassigned:", far_villages_df.shape[0])

    #Extract coords of these villages that are far away
    far_village_coords = list(zip(far_villages_df.GPS_lat, far_villages_df.GPS_long))

    
    #Calculate the new closest centroid for these villages which are far away from their current cluster centers
    new_clusters = nearest_centroid(village_coords = far_village_coords, centers_df_ = centers_df)

    #Merge old clusters with new clusters for far away villages
    #Merge these new cluster assignments back with original cluster assignments
    close_fake_villages = full_df.loc[(full_df.vill_centroid_dist < 5) | (full_df.cluster_id == 0)][["District","Block", "Village", "VillageCode", "GPS_lat", "GPS_long", "OOSCpredicted", "cluster_id"]]

    far_villages_new = pd.DataFrame({"District": far_villages_df.District, "Block":far_villages_df.Block, "Village":far_villages_df.Village, "VillageCode":far_villages_df.VillageCode, "GPS_lat":far_villages_df.GPS_lat, "GPS_long":far_villages_df.GPS_long, "OOSCpredicted":far_villages_df.OOSCpredicted, "cluster_id":new_clusters})

    merged_clusters_new = pd.concat([close_fake_villages, far_villages_new], ignore_index = True)

    writer = pd.ExcelWriter("Excel_results/Postprocessed2/Postprocessed2_far.xlsx")
    merged_clusters_new.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    writer.save()

    return
    
def postprocessing_addbyOOSC(cluster_results_, centers_df_):
    '''
    Adds in unclustered villages with 15 or more OOSC to existing clusters

    Parameters: 
    centers_df_: centers dataframe of existing clusters generated by compile_original_results() 
    or compute_centers() functions
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() 
    or postprocessing functions 
    '''

    #Extract all unclustered points with 15 or more OOSC children
    unclustered_to_cluster = cluster_results_[(cluster_results_.cluster_id == 0) & (cluster_results_.OOSCpredicted >= 15)]
    print("Number of villages to add to clusters based on number of OOSC:", unclustered_to_cluster.shape)

    #Data which will remain unclustered
    stay_unclustered = cluster_results_[(cluster_results_.cluster_id == 0) & (cluster_results_.OOSCpredicted < 15)]

    #Assign unclustered villages to closest centroid
    village_coords = list(zip(unclustered_to_cluster.GPS_lat, unclustered_to_cluster.GPS_long))
    clusters_new = nearest_centroid(village_coords = village_coords, centers_df_ = centers_df_)

    #Update the cluster_id of these villages
    unclustered_to_cluster.loc[:,["cluster_id"]] = clusters_new

    #Concatenate with villages which were already clustered and villages that remain unclustered
    already_clustered = cluster_results_[cluster_results_.cluster_id != 0]
    merged_data = pd.concat([already_clustered, unclustered_to_cluster, stay_unclustered], ignore_index = True)

    writer = pd.ExcelWriter("Excel_results/Postprocessed3/Postprocessed3a_addOOSC.xlsx")
    merged_data.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    writer.save()
    return

def nearest_centroid_dist(data_, centers_df_):
    village_coords = list(zip(data_.GPS_lat, data_.GPS_long))

    ''' 
    Calculates distance to nearest centroid for every village in data_ and stores the closest 
    cluster ID and the distance to that centroid in a dictionary

    Returns dictionary with keys = (village_index, nearest_centroid_index) and 
    values = distance to nearest centroid

    Parameters:
    data_: dataframe subsetted from cluster_results_ of villages to calculate nearest centroid for
    centers_df_: centers dataframe of existing clusters generated by compile_original_results() 
    or compute_centers() functions
    '''

    closest_centroid = {}
    for counter, village in enumerate(village_coords):
        dist_dict = {}
        for index, row in centers_df_.iterrows():
            dist_dict[row.cluster_id] = distance_calc(village, row.centroid)
        closest_centroid[(counter, min(dist_dict, key=dist_dict.get))] = dist_dict[min(dist_dict, key=dist_dict.get)]
        # print("Smallest distance for {}".format(counter),dist_dict[min(dist_dict, key=dist_dict.get)])
        # print("Smallest distance for {}".format(counter), min(dist_dict.values()))
    return closest_centroid


def postprocessing_addbydist(cluster_results_, centers_df_):
    '''Cluster enough unclustered villages to meet the 1800 village expansion goal
    Adds in villages in increasing order of distance to existing cluster centers

    Parameters:
    centers_df_: centers dataframe of existing clusters generated by compile_original_results() 
    or compute_centers() functions
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() 
    or postprocessing functions 
    '''
    #Drop 0th centroid from cluster centers
    centers_df_real = centers_df_[centers_df_.cluster_id != 0]

    #Divide dataset into already clustered villages and not clustered villages
    already_clustered = cluster_results_[cluster_results_.cluster_id != 0]
    unclustered_df = cluster_results_[cluster_results_.cluster_id == 0]

    #Add village index column to unclustered villages for cluster reassignment at end
    indices_df = pd.DataFrame({"village_indices":list(range(unclustered_df.shape[0]))})
    unclustered_df_indexed = pd.concat([unclustered_df.reset_index(drop=True), indices_df.reset_index(drop=True)], axis = 1)
    
    #Calcalate how many villages need to be clustered
    num_need_cluster = 1800 - cluster_results_[cluster_results_.cluster_id != 0].shape[0]
    print("Number of village that need to be clustered to reach 1800 village expansion goal:", num_need_cluster)

    '''Generate a dictionary with keys = (village_index, nearest_centroid_index) and 
    values = distance to nearest centroid
    '''
    print("Calculating closest centroid distance for every unclustered village. This step will take about a minute.")
    closest_centroid_dict = nearest_centroid_dist(data_ = unclustered_df, centers_df_ = centers_df_real)
    print("Done calculating distances and updating cluster IDs for newly clustered villages.")

    #Sorted dictionary by village distance to cluster center in increasing order
    villages_to_clusters = sorted(closest_centroid_dict, key=closest_centroid_dict.get, reverse=False)[:num_need_cluster]
    
    #Extracting village indices for villages being assigned to a cluster as list
    unzipped = list(zip(*villages_to_clusters))
    indices_to_cluster = list(unzipped[0])

    #Extracting the new cluster ID for villages being assigned to a cluster as list
    new_clusters = list(unzipped[1])

    #Concatenating village indices and new cluster IDs for villages being assigned to new clusters
    new_clusters_df = pd.DataFrame({"village_indices":indices_to_cluster, "new_clusters":new_clusters})

    #Update clusters for villages that have been selected to go from not being clustered to being clustered 
    for index, row in new_clusters_df.iterrows():
        village_index = row["village_indices"]
        unclustered_df_indexed.loc[unclustered_df_indexed["village_indices"] == village_index, "cluster_id"] = row.new_clusters


    #Drop village indexing column and concatenate newly clustered villages with already clustered villages
    unclustered_df_new = unclustered_df_indexed.drop(["village_indices"], axis = 1)
    merged_data = pd.concat([already_clustered, unclustered_df_new], ignore_index = True)

    #Write results to an excel file
    writer = pd.ExcelWriter("Excel_results/Postprocessed3/Postprocessed3b_addbydist.xlsx")
    merged_data.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    writer.save()

    return

def postprocessing_small_if_few(cluster_stats_, cluster_results_, centers_df_, cluster_vals_dropped_ = []):
    '''Dissolves and reclusters clusters with 4 or fewer villages if they have less than 100
    OOSC

    **Note: Doesn't recluster cluster_ID 4 even though it meets this criteria because 
    it will cross a lake if you do (CHANGE IF RUNNING ON NEW DATA)

    Parameters:
    clusters_stats_: cluster summary statistics dataframe generated by Kmeans/IP algorithm final_stats() 
    function or recompute_final_stats() function
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    centers_df_: centers dataframe generated by compile_original_results() or compute_centers() functions
    cluster_vals_dropped_: list of cluster_IDs that have been dropped, making cluster IDs non-continuous


    '''
    #Don't recluster cluster 4 because it will cross a lake
    print("NOT reclustering cluster_ID 4 even though it meets reclustering criteria, because it will cross a lake if it's reclustered")
    small_clusters = cluster_stats_[(cluster_stats_.Num_villages <= 4) & (cluster_stats_.Total_OOSC < 100) & (cluster_stats_.clust_num != 4) ]
    small_clust_ids = list(small_clusters.clust_num.values)

    #Select the clustering results for the villages in clusters with less than 4 villages
    small_clusters = cluster_results_.loc[cluster_results_.cluster_id.isin(small_clust_ids)]
    small_cluster_coords = list(zip(small_clusters.GPS_lat, small_clusters.GPS_long))
    print("Number of villages reassigned:", len(small_cluster_coords))

    #Centroids remaining once you dissolve small clusters
    centroid_df = centers_df_[~centers_df_.cluster_id.isin(small_clust_ids)]

    #Calculate nearest new centroid for villages which belong to a dissolved cluster
    new_clusters = nearest_centroid(village_coords = small_cluster_coords, centers_df_ = centroid_df)

    #Merge these new cluster assignments back with original cluster assignments
    large_clusters = cluster_results_.loc[~cluster_results_.cluster_id.isin(small_clust_ids)]
    small_clusters_new = pd.DataFrame({"District": small_clusters.District, "Block":small_clusters.Block, "Village":small_clusters.Village, "VillageCode": small_clusters.VillageCode, "GPS_lat":small_clusters.GPS_lat, "GPS_long":small_clusters.GPS_long, "OOSCpredicted":small_clusters.OOSCpredicted, "cluster_id":new_clusters})
    merged_clusters_new = pd.concat([large_clusters, small_clusters_new], ignore_index = True)

    #Update cluster values dropped list to include dissolved clusters from this postprocessing
    cluster_vals_dropped_ += small_clust_ids

    clusters_dropped_df = pd.DataFrame({"Clusters_dropped":cluster_vals_dropped_})
    writer = pd.ExcelWriter("Excel_results/Postprocessed5/Postprocessed5_smallfew.xlsx")
    merged_clusters_new.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    clusters_dropped_df.to_excel(excel_writer = writer, sheet_name = 'Clusters_dropped')
    writer.save()

    return

def renumber_clusters(cluster_results_):
    '''Renumber clusters to have cluster IDs that are continuous
    Accounts for clusters that were dropped in postprocessing

    Parameters:
    cluster_results_: Village level cluster results dataframe generated by compile_original_results() or postprocessing functions 
    '''
    ids = list(set(cluster_results_.cluster_id.values))
    c_to_id = dict()
    #Dictionary with keys = current_cluster_id and value = new_cluster_id
    for counter, id_val in enumerate(ids):
        c_to_id[id_val] = counter

    #Renumber clusters using new continuous cluster IDs
    cluster_results_new = cluster_results_.copy()
    for index, row in cluster_results_new.iterrows():
        new_cluster_id = c_to_id[row.cluster_id]
        cluster_results_new.loc[index, "cluster_id"] = new_cluster_id

    writer = pd.ExcelWriter("Excel_results/Renumbered/Final_clusters.xlsx")
    cluster_results_new.to_excel(excel_writer = writer, sheet_name = 'Cluster_assignments')
    writer.save()

    return
